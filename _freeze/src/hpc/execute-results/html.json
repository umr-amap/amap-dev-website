{
  "hash": "76b62890993146722ca2a5d4ba2600ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"High Performance Computing\"\nexecute:\n  eval: false\n---\n\n\n\n\n\n# What is HPC ?\nHigh performance computing (HPC) is the practice of aggregating (\"clustering\") computing resources that work in parallel to gain greater performance than a single workstation and process massive multi-dimensional data sets.\n\nA standard computing system solves problems primarily by using serial computing. It divides the workload into a sequence of tasks and then runs the tasks one after the other on the same processor. On the other hand, HPC uses parallel processing, which divides the workload into tasks that run simultaneously on multiple processors.\n\nHPC computer systems are characterized by their high-speed processing power, high-performance networks, and large-memory capacity.\n\nWhile a laptop or desktop with a 3 GHz processor can perform around 3 billion calculations per second (~3.10^9), HPC solutions that can perform quadrillions of calculations per second (~10^{15}).\n\n## Do you need HPC ?\nThere are many cases where you might need to use HPC resources.\n\n### You need to run a lot of simulations\nLet's say you have an analysis that you performed on a single plot, on one year. You want to extend this analysis to all the plots and all the years of your study.\nYou'll probably find yourself in this type of situation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplots <- c('P01', 'P02', ...)\nyears <- c(2000, 2001, ...)\n\nfor (plot in plots) {\n    for (year in years) {\n        data <- get_data(plot, year)\n        data <- preprocess_data(data, arguments...)\n        results <- run_analysis(data, arguments...)\n        write_results(results, plot, year)\n    }\n}\n```\n:::\n\n\n\n\nThis will require you to run the same analysis many times. If the analysis is long to run, all the analyses being independant, you might want to use HPC to run all the simulations in parallel.\n\nYou need to modify your script so that it uses input arguments :\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nargs <- comman#| dArgs(trailingOnly = TRUE)\nplot <- args[1]\nyear <- as.numeric(args[2])\n\ndata <- get_data(plot, year)\ndata <- preprocess_data(data, arguments...)\nresults <- run_analysis(data, arguments...)\nwrite_results(results, plot, year)\n```\n:::\n\n\n\n\n\nand then launch the script concurrently for all the plots and years using [`Rscript`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/Rscript) and [`xargs`](https://www.man7.org/linux/man-pages/man1/xargs.1.html) :\n```bash\nfor plot in P01 P02 ...; do\n    for year in 2000 2001 ...; do\n        echo $plot $year\n    done\ndone | xargs -n 2 -P 4 Rscript my_script.R\n```\n\n### You need to run simulations that are very long\nYou might want to use parallelization to speed up the computation of a single simulation. [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [OpenMP](https://en.wikipedia.org/wiki/OpenMP) are two libraries that can help you to parallelize your code.\n\nYou can also access to very powerful hardware (typically [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)) that are not available on your computer, and that can speed up the computation of your simulations.\n\n### You need to run simulations that are very memory intensive\nLet's say that you developed a script on your computer that run on a small domain. You want to extend this script to a larger domain but you run out of memory on your computer. You have two options here to run your code on HPC:\n\n- Run your script on a node with more memory (some HPC nodes have memory up to ~1 To)\n- Use parallel computing to distribute the memory usage on multiple nodes\n\n### Advantages of using HPC\nThere are a wide range of advantages to using HPC, including:\n\n- ___Performance___: Very fast computation !!! This can help you to run simulations that would be impossible to run on a single workstation.\n- ___Scalability___: Ability to run many simulations in parallel. This can be useful for running many simulations with different parameters, or for running the same simulation many times.\n- ___Reliability___: HPC systems are designed to be reliable with a high percentage of availability. The machine is delocalized on a dedicated site, with a dedicated team to maintain it.\n- ___Cost___: HPC systems are shared resources, so you only pay for what you use. This can be more cost-effective than buying a high-end workstation.\n- ___Support___: HPC systems are usually supported by a team of experts who can help you to optimize your code and run your simulations.\n- ___Science focused___: you don't need to care about system administration and updating the system. You can focus on your science.\n\n### Disadvantages of using HPC\nThere are also some disadvantages to using HPC, that you should be aware of:\n\n- ___Learning curve___: HPC systems can be complex to use (shared machine, multiple storage spaces, job scheduler, linux machine), and you may need to learn new tools and techniques to use them effectively.\n- ___Limited resources___: HPC systems are shared resources, so you may not always be able to access the resources you need exactly when you need them.\n- ___Data transfer___: Transferring data to and from an HPC system can be slow, especially if you are working with large data sets.\n\n## What resources are available ?\nAt AMAP, we have access to several HPC resources. All the resources listed here are available freely to AMAP members (although institutes might contribute financialy, users are not requested to pay for computing hours or storage) and you can have additional support from POLIS-HPC team.\n\nOther options for HPC on the cloud (plateforms like AWS, Azure, Google Cloud) are also available, but are not covered here. These plateform are not free and are not eligible to any kind of support from POLIS.\n\n### Meso-center `ISDM-Meso` {#sec-mesolr}\nHosted at [CINES](https://www.cines.fr/), this regional Meso-center [`Meso@LR`](https://meso-lr.umontpellier.fr/) provides access to HPC resources for the Occitanie region. Although access to this resource is not free, an existing agreement between CIRAD and ISDM makes it free for researchers and accessible upon request only (most likely within a certain limit of computing hours, but we have never been able to verify that :smile:).\n\n#### Description of the cluster\nTechnical specificities include:\n\n- 308 computational nodes of 28 cores each (bi processors Intel Xeon E5-2680 v4 2,4 Ghz (broadwell) 2×14 coeurs/noeud), 128 Go RAM\n- 2 nodes with big memory (`bigmem`) with 112 cores, 3To RAM each\n- 2 nodes with GPU and 52 CPU cores (bi-processors 26 cores) Poweredge R740 embedding RTX6000\n- 1,3Po storage, including 1Po fast storage on Lustre\n- `SLURM` workload manager\n\n#### How to open an account\nGetting an account to `Meso@LR` is usualy quite fast (~1 day). You only need to be part of AMAP and have an email adress from CIRAD.\n\nYou must send a mail to [Bertrand Pitollat](mailto:bertrand.pitollat@cirad.fr) and the HPC team at AMAP (currently the contact person being [T. Arsouze](mailto:thomas.arsouze@cirad.fr)) with the following requirements:\n\n- explain who you are (student or post-doc or ... ? supervisor ? one sentence about your research)\n- request to be attached to the group `d_cirad_amap` (this is the group of AMAP members)\n- fill the `charte informatique` available [here](https://meso-lr.umontpellier.fr/wp-content/uploads/2022/08/CharteCALCUL-NOM-Prenom.pdf)\n\n### GENCI\nIf you need larges resources (i.e. multi-GPU workflow, or > ~1000 CPUs working in parallel), this is probably your best option.\n\n[GENCI](https://www.genci.fr/) is the French national HPC agency. It provides access to [several HPC resources in France](https://www.genci.fr/services/moyens-de-calcul):\n\n- [CINES](https://www.genci.fr/centre-informatique-national-de-lenseignement-superieur-cines) (Montpellier)\n- [IDRIS](https://www.genci.fr/centre-informatique-national-de-lenseignement-superieur-idris) (Saclay)\n- [TGCC](https://www.genci.fr/tres-grand-centre-de-calcul-du-cea-tgcc) (Bruyères-le-Châtel)\n\nAccess and computing resources are free for academic.\n\n#### Description of the clusters\n`Adastra` cluster at CINES is the most recent cluster available to GENCI users. It provides the best option for GPU computing, although it is powered by `AMD` processors (EPYC Trento). APUs AMD MI300A are also available.\n\n`Jean-Zay` cluster at IDRIS has recently been updated (09/2024) to the most powerfull `NVIDIA` GPUs available on the market (H100)\n\n`Joliot-Curie` cluster at TGCC provides access `Nvidia V100 SXM2` GPUs and Intel CPUs. Note that you can also have access to `Quantique plateform`.\n\n#### How to open an account ?\nAnyone at AMAP can open an account to GENCI resources. The process is a bit longer than for `Meso@LR` but is still quite fast (< 1 week) if you don't ask for too much resources.\n\n::: {.callout-warning}\nBefore submitting any project, we highly recommand you to contact the POLIS HPC-team at AMAP (currently T. Arsouze, P. Verley and D. Lamonica) to discuss your needs and the best way to use the resources.\n:::\n\nThere are two ways to access GENCI resources:\n\n- \"Accès dynamiques\" (fast access) : if your needs are < 500 000 h CPU and 50 000 h GPU (eq. V100) for 1 year. This is a simplified access, only a one paragraph description of the scientific context is required and no scientific or technical review is done.\n- \"Accès réguliers\" (regular access) : if your needs are too important for dynamical access, then you must go for regular access. You can request for resources only two times per year, and provide a detailed description of your project, both scientifically and technically.\n\n- You first need to login to the [edari website](https://www.edari.fr/user/login) (DARI: Demande d'Attribution de resources Informatiques) using RENATER federation.\n- First you need to ask for resources:\n  - Go to \"Demande de resources\" -> \"Créer ou renouveler un dossier de demande d'heures.\" -> \"Création d'un nouveau dossier\"\n  - Then fill the forms:\n    - Form n°2 (resources) : correctly fill the type of resources you need, the number of hours and the amount of storage needed. If you don't know what to put here, ask the HPC team at AMAP.\n    - Form n°3 (Structure de recherche) : choose UMR5120, \"Identifiant RNSR\" 200317641S\n    - Form n°5 (Soutien du projet) : you have access to mesocenter `Meso@LR`\n- Then you need to an account to the supercomputer:\n  - \"Demande de compte sur les calculateurs\" -> \"Demande de rattachement à un dossier de demande d'heures\" and mention the project for resources you just created.\n  - \"Demande de compte sur les calculateurs\" -> \"Demande d'ouverture de compte\". At some point you will need to provide an IP address of your computer: put `194.199.235.81`, and your \"FQDN (Fully Qualified Domain Name) obligatoire associé à l'adresse IP\" : put `cluster-out.cirad.fr`.\n- Finally, you... wait ! You will receive an email when your account is created and you project validated.\n\n## How to use HPC clusters ?\nTo efficiently use HPC clusters, you'll need to follow a few steps.\n\n- connect to the cluster and transfer your files\n- create your computation environment in the cluster\n- learn how to submit jobs\n\n::: {.callout-warning}\nThe following steps are general and should apply to any HPC resources. We provide below examples for `Meso@LR` cluster. However, before any action, you ___MUST___ read the documentation of the cluster you are using:\n\n- [Meso@LR](https://hpc-lr.umontpellier.fr/documentation-utilisateurs/)\n- [Adastra at CINES](https://dci.dci-gitlab.cines.fr/webextranet/)\n- [Jean-Zay at IDRIS](http://www.idris.fr/jean-zay/)\n:::\n\n### Connect and transfer files to the cluster\n#### Connection\n##### Using a terminal\nThe most obvious and common way to connect to a remote machine (here your HPC cluster) is to use the [`ssh` protocol](https://www.techtarget.com/searchsecurity/definition/Secure-Shell). For exemple, for `Meso@LR`cluster, in a terminal application, you can type:\n\n```bash\nssh -X my_login@muse-login.meso.umontpellier.fr\n```\n\nTo avoid typing your password each time you connect, you can use a [ssh key](https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server).\n\n::: {.callout-note}\nAlthough considered as secure, ssh keys are sometimes not accepted on some HPC clusters. For example `adastra` does not accept ssh key.\n:::\n\nYou can [use basic linux commands](https://www.hostinger.com/tutorials/linux-commands) to navigate through the cluster.\n\nYou are now connected to the `frontal` or `login` node of the cluster. This is the node where you can submit jobs, transfer files, and prepare your computation environment. This is ___NOT___ the node where your jobs will be executed and where you can do your computations.\n\n##### Using an IDE\nYou can also use an IDE (Integrated Development Environment - code editor that embeds multiple tools for developers) to connect to the cluster. For example `VSCode` provides a powerfull plugin to [develop on remote machines](https://code.visualstudio.com/docs/remote/ssh). This allows you to navigate, edit and run your code on the cluster directly from your IDE.\n\n`PyCharm` also provides a plugin to [develop on remote machines](https://www.jetbrains.com/help/pycharm/remote-development-starting-page.html).\n\nHowever, to our knowledge, no such solution exists for `RStudio`.\n\n#### Transfer files\nTo run your code on the cluster, you need first to transfer your files. You can use multiple tools to transfer files:\n\n##### `scp` & `rsync`\nIf you feel at ease with a terminal, you can use `scp` or `rsync` commands to transfer files. For example, to transfer a file from your computer to the cluster, you can type:\n\n```bash\n# single file\nscp /where/my_file/is/my_file.txt my_login@muse-login.meso.umontpellier.fr:/where/to/put/it/\n# directory\nscp -r /where/my_directory/is/my_directory my_login@muse-login.meso.umontpellier.fr:/where/to/put/it/\n```\n\nThis means that you have to know beforehand the path to the directory on the cluster that you want to put your files in.\n\n[`rsync`](https://hpc-lr.umontpellier.fr/wp-content/uploads/2017/05/rsync.txt) is a more powerfull tool that allows you to synchronize directories between your computer and the cluster. It has a lot of options that you can use to customize the synchronization, and allows you to transfer only the files that have been modified since the last synchronization (very useful when you have a lot of files to transfer or when you have a slow and instable connection).\n\n##### Filezilla\n[`Filezilla`](https://filezilla-project.org/) is a client for transfering files among machines. It is very easy to use and provides a graphical interface.\n\nYou can find the \"Quickconnect\" bar at the top of the main window, and you need to provide the address of the \"Host\" : `muse-login.meso.umontpellier.fr`, your \"Username\" and \"Password\". You can also save your connection for later use : \"File\" -> \"Copy current connection to Site Manager…\".\n\n##### Where to put my files ?\nHPC clusters usually have multiple places where to put your files. Here are the ones for `Meso@LR`:\n\n- `/home/my_login` : this is your home directory. You can put your scripts and small files here. This directory is backed up.\n- `/lustre/scratch` : this is a temporary directory with very fast access where you can put your large files and data. This directory is not backed up and can be erased after 2 months. This space is recommended for running your computations that can require large amount of data.\n- `/nfs/work` : this is a directory where you can put your input / output data or executable files. This directory is backed up and can be used to store your data for a long time.\n\n### Create your computation environment\nHPC clusters usually provide a lot of scientific softwares and libraries that you can use. They are installed in a module system that you can load and unload. Some useful commands are:\n\n- `module avail` : list all the available modules\n\n```bash\n$ module avail\n\n------------------------------------------------------------------------- /usr/share/Modules/modulefiles -------------------------------------------------------------------------\ndot         module-git  module-info modules     null        use.own\n\n-------------------------------------------------------------------- /trinity/shared/modulefiles/modulegroups --------------------------------------------------------------------\nbioinfo-cirad bioinfo-ifb   cv-admin      cv-advanced   cv-local      cv-standard   local\n\n----------------------------------------------------------------------- /trinity/shared/modulefiles/local ------------------------------------------------------------------------\nautomake/1.16.1             geos/3.10.6                 ImageMagick/7.0.3-10        openjpeg/2.4.0              R/3.6.1                     sqlite/3.38.1\nCast3M/2019                 geos/3.7.2                  intel/opencl-1.2-6.4.0.37   openslide/3.4.1             R/3.6.1-tcltk               squashfs/4.3\ncgal/5.2                    ghmm/1.0                    JDK/11.0.15                 phast/1.5                   R/3.6.3                     swig/3.0.11\ncmake/3.22.0                git/2.35.1                  JDK/jdk.8_x64               pi4u/torc                   R/4.0.2                     vmd/1.9.3\n...\ndawgpaws/1.0                       ImageMagick/7.0.11-14              pandoc/2.14.0.1                    skif/1.2                           yak/0.1\ndeepgp/20220929-singularity        insilicoseq/1.5.4                  pantherscore/1.03                  SLR/20240610                       yulab-smu/20230217\ndeeptmhmm/1.0.20                   interproscan/5.41-78.0             pantherscore/2.2                   slurm_drmaa/1.1.1                  zlib/1.2.8\ndeepTools/3.5.1                    interproscan/5.48-83.0             parallel/20220922                  smrtlink/10.1.0.119588\ndeepvariant/1.1.0                  interproscan/5.67-99.0             parsnp/1.7.4-conda                 snakemake/5.12.3\n```\n- `module load my_module` : load a module, e.g. `module load R/packages/4.3.1-IFB`\n- `module unload my_module` : unload a module, e.g. `module unload R/packages/4.3.1-IFB`\n- `module list` : list all the loaded modules in your environment, e.g.:\n```bash\n$ module load R/packages/4.3.1-IFB\n$ module list\nCurrently Loaded Modulefiles:\n  1) bioinfo-ifb            2) r/4.3.1                3) R/packages/4.3.1-IFB\n```\nYou can see that `R/packages/4.3.1-IFB` (recommend for `R` users) is loaded with some dependecies.\n- `module purge` : unload all the loaded modules\n\nTo access modules, you need to do some configuration first:\n```bash\nnano ~/.bashrc    # nano is a text editor\n\n# add the following lines at the end of the file\n\n# module\nMODULEPATH=$MODULEPATH:/nfs/work/agap_id-bin/modulefiles\nexport MODULEPATH\n\n# User specific environment and startup programs\nPATH=$PATH:$HOME/.local/bin:$HOME/bin\nexport PATH\n\n# R_LIBS_USER\nR_LIBS_USER=~/R/x86_64-pc-linux-gnu-library/4.3.1\nexport R_LIBS_USER\n\n```\n\n#### Install additional R packages\nFirst, be aware that a lot of packages are already installed inside the `R/packages/4.3.1-IFB` module. If the module you need is not available, you have two solutions:\n- You can install packages in your home directory. If you need to install a package that requires compilation (such as `brms` for example), you'll need to provide information on how to do that:\n```bash\n$ cd ~       # back to your HOME directory\n$ mkdir .R   # create a directory for R packages configuration\n$ echo \"CXX14 = g++ -fPIC\\nCXX14FLAGS = -O3\\nCXX14PICFLAGS = -fpic\\nCXX14STD = -std=gnu++1y\\n\" > ~/.R/Makevars\n# You can now install your packages\n$ R --vanilla\n> install.packages('brms')\nInstallation du package dans ‘/nfs/work/agap_id-bin/img/R/4.3.1-IFB/lib’\n(car ‘lib’ n'est pas spécifié)\nAvis dans install.packages(\"brms\") :\n  'lib = \"/nfs/work/agap_id-bin/img/R/4.3.1-IFB/lib\"' ne peut être ouvert en écriture\nVoulez-vous plutôt utiliser une bibliothèque personnelle ? (oui/Non/annuler) oui\n```\n\nThis will install your packages in the `$HOME/R/x86_64-pc-linux-gnu-library/4.3.1` directory.\n\n- You can also use [conda](https://docs.conda.io/en/latest/) to create a dedicated environment and install additional packages. Although mostly used for installing python packages, conda can also be used to install R packages (search search for packages [here](https://anaconda.org/search?q=r-)).\n\n#### Install additional python packages\nThe recommanded way to work with Python on the cluster is by using an environment manager. While there are many available (conda, virtualenv, pipenv, poetry, ...), we recommand to use `conda` as it is the most widely used by the scientific community AND it is already installed on the cluster.\n\n```bash\nmodule load anaconda/python3.8\n# Don't pay attention to the python version, you'll be able to create your own environment with the version you need.\n```\n\nWe recommand to tune the `conda` configuration file to work with the cluster:\n```bash\nnano ~/.condarc   # This will create / open the file .condarc in your HOME directory in the text editor\n# Then in the .condarc file, add the following lines:\nchannels:   # list the channels where conda will search for packages\n  - nodefaults\n  - conda-forge\nsolver: libmamba   # this is the most efficient solver for package dependencies. Now by default, but you can specify it here.\nchannel_priority: strict  # Force conda to search for packages in the order you specified in the channels list\nssl_verify: false   # Specific to meso@lr, the SSL certificate is not valid, so you need to disable SSL verification\nenvs_dirs:    # The place where you want to store your environments. If not specified, conda will create a directory in your HOME directory, but if you deal with many large environments, you might want to store them in the work directory.\n    - /path/where/to/store/your/envs\n```\n\nThen you can create and access your environments:\n```bash\n# create an environment\nconda create -n my_env python=3.11\n# activate the environment and install packages\nconda activate my_env\nconda install numpy pandas ...\n# deactivate the environment\nconda deactivate\n```\n\n### Submit jobs\nAll HPC clusters use a job scheduler to manage the resources and the jobs submitted by users. The most common job scheduler is `SLURM` (Simple Linux Utility for Resource Management). This is the scheduler use at `meso@lr`, but also `adastra` and `jean-zay`.\n\n#### The SLURM scheduler\nYou can find the documentation for `SLURM` [here](https://slurm.schedmd.com/).\n\nYou can run your job in 2 modes:\n- direct / interactive mode, using the command `srun` : for example `srun --pty bash` will open a bash session on a compute node.\n- batch mode, using the command `sbatch` : you need to write a script that will be executed by the scheduler.\n\nIt is recommended to use the batch mode for long and heavy computations, and the interactive mode for testing your code.\n\n#### How to write a SLURM script\nWe refer to `SLURM` documentation to see all the options available, but we list here the most basic and useful ones to write a batch script.\n\n|Slurm long format |\tSlurm short format |\tDescription |\n|-------------|:-------|:-------------------|\n|--job-name\t| -J |\tName of the job |\n|--account | | Name of the account to which the job is charged |\n|--nodes |\t-N\t| Number of nodes required |\n|--ntasks\t| -n\t| Number of tasks of your job |\n|--ntasks-per-node |  | Number of tasks per node (should be equal to the total number of tasks divided by the number of nodes) |\n|--ntasks-per-core |  | Number of tasks per core (usually 1) |\n|--partition\t| -p\t| Name of the partition you want to use |\n|--mem |  | Memory required by the job in Mb (if not specified, then the whole node memory is allocated)|\n|--time\t| -t |\tWall time requested (HH:MM:SS), i.e. maximum time for your job to run. |\n|--output\t| -o |\tOutput file where the standard output will be written. By default, slurm-%j.out or '%j' is the job number |\n|--error\t| -e |\tError file where the standard error will be written. By default, slurm-%j.err or '%j' is the job number |\n|--mail-user |\t|\tEmail address to send notifications |\n\n\n#### How to submit and monitor a job\n#### Different partitions available\nDifferent types of resources are available on the cluster, as listed in @sec-mesolr:\n\n- __CPU__ node. Partition name : `agap_long`, `agap_normal`, `agap_short`\n\n- __GPU__ node. Partition name : `agap_gpu`\n\n- __Big memory__ node. Partition name : `agap_bigmem`\n\nYou can find the list of partitions available on the cluster with the command `sinfo` (including time limits and number of nodes available).\n\n# Concrete examples\nHere is a very simple script that you can adapt to your needs. You can fill additional `SLURM` options, and also the modify the call to your script.\n\n```bash\n#!/bin/bash                             # mandatory first line. This is a bash script\n#SBATCH --job-name=\"<job_name>\"         # name of the job\n#SBATCH --partition=partition_name      # name of the partition you want to use\n#SBATCH --nodes=1                       # number of nodes required\n#SBATCH --exclusive                     # ask SLURM to reserve the whole nodes. This is not desired if you only need a few cores.\n#SBATCH --time=1:00:00                  # wall time requested (HH:MM:SS)\n\nmodule purge                            # clean the environment\nmodule load R/packages/4.3.1-IFB        # load the software you need\n\nset -v -x                               # useful to debug your script\n\n# Where your project is located, and where you will run it\nMY_PROJ_DIR=\"/home/arsouzet/projects/AMAP/arsouze/runs_StormR/\"\nMY_PROJ_SCRATCH_DIR=\"/home/arsouzet/scratch/runs_StormR/$SLURM_ARRAY_TASK_ID\"\n\n# Your script has 2 arguments: the input file and the output file\ninput_filename=\"my_input_file.nc\"\noutput_filename=\"my_output_file.tiff\"\n\n# Manage the directory where you will run your analysis\nif [ ! -d \"${MY_PROJ_SCRATCH_DIR}\" ]\nthen\n    mkdir ${MY_PROJ_SCRATCH_DIR}\nfi\n\ncd ${MY_PROJ_DIR}\ncp my_analysis.R ${input_filename} ${MY_PROJ_SCRATCH_DIR}\ncd ${MY_PROJ_SCRATCH_DIR}\n\n# Run the R script\nR CMD BATCH --vanilla \"--args ${input_filename} ${output_filename}\" my_analysis.R\n# Save the output file to a safe place\nmv ${output_filename} ${MY_PROJ_DIR}\n```\n\n## Running a script using GPU on `meso@lr`\nTo run a script using GPU on `meso@lr`, you need a little bit more tweaking (while it might be easier on adastra for example).\n\nBy default, if you install a package that requires GPU, it will not work on `meso@lr` because the GPU is not available on the login node. You first need to install your package on a GPU compute node.\n\n> warning\n> To run a script on a GPU node, please add the following to your `sbatch` scripts:\n> ```bash\n> module () {\n>     eval `/usr/bin/modulecmd bash $*`\n> }\n> ```\n> This will allow you to use the `module` command in your scripts.\n\n\n### Submit an interactive job on GPU node [#interactive-gpu]\nYou first need to connect to a GPU node to install your package. You can do this by submitting an interactive job on a GPU node. Here is an example of a script that you can use to submit an interactive job on a GPU node:\n```bash\nsrun -A AGAP -p agap_gpu --gres gpu:a100:1 -t 00:30:00 -c 1 -J interactive1 --pty /bin/bash -i\n```\nThis means you are asking for 1 GPU of type `a100` for 30 minutes, on `agap_gpu` partition, in interactive mode.\n\nOnce you are connected to the GPU node, you must check the version of the `nvidia` driver with the command `nvidia-smi`. You can then exit the GPU node with `exit`.\n\n### Install you environment\nInstall `pytorch` (or `tensorflow` or any other package that requires GPU) in a dedicated environment which is compatible with the `nvidia` driver version seens in #interactive-gpu section. Install also additional dependencies needed.\n\n### Submit a job on GPU node\nYou can now submit a job on a GPU node. Here is an example of a script that you can use to submit a job on a GPU node:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --account=agap\n#SBATCH --partition=agap_gpu\n#SBATCH --output=/my/output/path/output.log\n#SBATCH --time=8:00:00\n#SBATCH --ntasks=1\n#SBATCH --gres=gpu:a100:1    # we only need 1 GPU\n#SBATCH --mem=32000\n\nmodule () {\n    eval `/usr/bin/modulecmd bash $*`\n}\n\nmodule purge\nmodule load ... # load the software you need\n\ncd /lustre/my_account/prg/\n./runThroughConfig_SSL_HP.sh\n```\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}