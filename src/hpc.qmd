---
title: "HPC"
execute:
  eval: false
---

# What is HPC ?
High performance computing (HPC) is the practice of aggregating ("clustering") computing resources that work in parallel to gain performance greater than that of a single workstation and process massive multi-dimensional data sets.

A standard computing system solves problems primarily by using serial computing—it divides the workload into a sequence of tasks and then runs the tasks one after the other on the same processor. On the other hand, HPC uses parallel processing, which divides the workload into tasks that run simultaneously on multiple processors.

HPC computer systems are characterized by their high-speed processing power, high-performance networks, and large-memory capacity.

While a laptop or desktop with a 3 GHz processor can perform around 3 billion calculations per second (~3.10^9), HPC solutions that can perform quadrillions of calculations per second (~10^{15}).

## Do you need HPC ?
There are many cases where you might need to use use HPC ressources.

### You need to run a lot of simulations
Let's say you have an analysis that you performed on a single plot, on one year. You want to extend this analysis to all the plots and all the years of your study.
You'll probably find yourself in this type of situation:
```{r}
plots <- c('P01', 'P02', ...)
years <- c(2000, 2001, ...)

for (plot in plots) {
    for (year in years) {
        data <- get_data(plot, year)
        data <- preprocess_data(data, arguments...)
        results <- run_analysis(data, arguments...)
        write_results(results, plot, year)
    }
}
```
This will require you to run the same analysis many times. If the analysis is long to run, all the analyses being independant, you might want to use HPC to run all the simulations in parallel.

You need to modify your script so that it uses input arguments :
```{r}
args <- commandArgs(trailingOnly = TRUE)
plot <- args[1]
year <- as.numeric(args[2])

data <- get_data(plot, year)
data <- preprocess_data(data, arguments...)
results <- run_analysis(data, arguments...)
write_results(results, plot, year)
```

and then launch the script concurrently for all the plots and years using [`Rscript`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/Rscript) and [`xargs`](https://www.man7.org/linux/man-pages/man1/xargs.1.html) :
```bash
for plot in P01 P02 ...; do
    for year in 2000 2001 ...; do
        echo $plot $year
    done
done | xargs -n 2 -P 4 Rscript my_script.R
```

### You need to run simulations that are very long
You might want to use parallelization to speed up the computation of a single simulation. [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [OpenMP](https://en.wikipedia.org/wiki/OpenMP) are two libraries that can help you to parallelize your code.

You can also access to very powerful hardware (typically [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)) that are not available on your computer, and that can speed up the computation of your simulations.

### You need to run simulations that are very memory intensive
Let's say that you developed a script on your computer that run on a small domain. You want to extend this script to a larger domain but you run out of memory on your computer. You have two options here to run your code on HPC:
- Run your script on a node with more memory (some HPC nodes have memory up to ~1 To)
- Use parallel computing to distribute the memory usage on multiple nodes

### Advantages of using HPC
There are a wide range of advantages to using HPC, including:
- ___Performance___: Very fast computation !!! This can help you to run simulations that would be impossible to run on a single workstation.
- ___Scalability___: Ability to run many simulations in parallel. This can be useful for running many simulations with different parameters, or for running the same simulation many times.
- ___Reliability___: HPC systems are designed to be reliable with a high percentage of availability. The machine is delocalized on a dedicated site, with a dedicated team to maintain it.
- ___Cost___: HPC systems are shared resources, so you only pay for what you use. This can be more cost-effective than buying a high-end workstation.
- ___Support___: HPC systems are usually supported by a team of experts who can help you to optimize your code and run your simulations.
- ___Science focused___: you don't need to care about system administration and updating the system. You can focus on your science.

### Disadvantages of using HPC
There are also some disadvantages to using HPC, that you should be aware of:
- ___Learning curve___: HPC systems can be complex to use (shared machine, multiple storage spaces, job scheduler, linux machine), and you may need to learn new tools and techniques to use them effectively.
- ___Limited resources___: HPC systems are shared resources, so you may not always be able to access the resources you need exactly when you need them.
- ___Data transfer___: Transferring data to and from an HPC system can be slow, especially if you are working with large data sets.

## What ressources are available ?
At AMAP, we have access to several HPC ressources. All the ressources listed here are available freely to AMAP members (although institutes might contribute financialy, users are not requested to pay for computing hours or storage) and you can have additional support from POLIS-HPC team.

Other options for HPC on the cloud (plateforms like AWS, Azure, Google Cloud) are also available, but are not covered here. These plateform are not free and are not eligible to any kind of support from POLIS.

### Meso-center `ISDM-Meso`
Hosted at [CINES](https://www.cines.fr/), this regional Meso-center [`Meso@LR`](https://meso-lr.umontpellier.fr/) provides access to HPC ressources for the Occitanie region. Although access to this ressource is not free, an existing agreement between CIRAD and ISDM makes it free for researchers and accessible upon request only (most likely within a certain limit of computing hours, but we have never been able to verify that :smile:).

#### Description of the cluster
Technical specificities include:
- 308 computational nodes of 28 cores each (bi processors Intel Xeon E5-2680 v4 2,4 Ghz (broadwell) 2×14 coeurs/noeud), 128 Go RAM
- 2 nodes with big memory (`bigmem`) with 112 coeurs, 3To RAM each
- 2 noeuds with GPU and 52 CPU cores (bi-processors 26 cores) Poweredge R740 embedding RTX6000
- 1,3Po storage, including 1Po fast storage on Lustre
- `SLURM` workload manager

#### How to open an account
Getting an account to `Meso@LR` is usualy quite fast (~1 day). You only need to be part of AMAP and have an email adress from CIRAD.

You must send a mail to [Bertrand Pitollat](mailto:bertrand.pitollat@cirad.fr) and the HPC team at AMAP (currently the contact person being [T. Arsouze](mailto:thomas.arsouze@cirad.fr)) with the following requirements:
- explain who you are (student or post-doc or ... ? supervisor ? one sentence about your research)
- request to be attached to the group `d_cirad_amap` (this is the group of AMAP members)
- fill the `charte informatique` available [here](https://meso-lr.umontpellier.fr/wp-content/uploads/2022/08/CharteCALCUL-NOM-Prenom.pdf)

### GENCI
If you need larges ressources (i.e. multi-GPU workflow, or > ~1000 CPUs working in parallel), this is probably your best option.
[GENCI](https://www.genci.fr/) is the French national HPC agency. It provides access to [several HPC ressources in France](https://www.genci.fr/services/moyens-de-calcul):
- [CINES](https://www.genci.fr/centre-informatique-national-de-lenseignement-superieur-cines) (Montpellier)
- [IDRIS](https://www.genci.fr/centre-informatique-national-de-lenseignement-superieur-idris) (Saclay)
- [TGCC](https://www.genci.fr/tres-grand-centre-de-calcul-du-cea-tgcc) (Bruyères-le-Châtel)

Access and computing ressources are free for academic.

#### Description of the clusters
`Adastra` cluster at CINES is the most recent cluster available to GENCI users. It provides the best option for GPU computing, although it is powered by `AMD` processors (EPYC Trento). APUs AMD MI300A are also available.

`Jean-Zay` cluster at IDRIS has recently been updated (09/2024) to the most powerfull `NVIDIA` GPUs available on the market (H100)

`Joliot-Curie` cluster at TGCC provides access `Nvidia V100 SXM2` GPUs and Intel CPUs. Note that you can also have access to `Quantique plateform`.

#### How to open an account ?
Anyone at AMAP can open an account to GENCI ressources. The process is a bit longer than for `Meso@LR` but is still quite fast (< 1 week) if you don't ask for too much ressources.

> warning
> Before submitting any project, we highly recommand you to contact the POLIS HPC-team at AMAP (currently T. Arsouze, P. Verley and D. Lamonica) to discuss your needs and the best way to use the ressources.

There are two ways to access GENCI ressources:
- "Accès dynamiques" (fast access) : if your needs are < 500 000 h CPU and 50 000 h GPU (eq. V100) for 1 year. This is a simplified access, only a one paragraph description of the scientific context is required and no scientific or technical review is done.
- "Accès réguliers" (regular access) : if your needs are too important for dynamical access, then you must go for regular access. You can request for ressources only two time per year, and provide a detailed description of your project, both scientificaly and technicaly.

- You first need to login to the [edari website](https://www.edari.fr/user/login) (DARI: Demande d'Attribution de Ressources Informatiques) using RENATER federation.
- First you need to ask for ressources:
  - Go to "Demande de ressources" -> "Créer ou renouveler un dossier de demande d'heures." -> "Création d'un nouveau dossier"
  - Then fill the forms:
    - Form n°2 (Ressources) : correctly fill the type of ressources you need, the number of hours and the amount of storage needed. If you don't know what to put here, ask the HPC team at AMAP.
    - Form n°3 (Structure de recherche) : choose UMR5120, "Identifiant RNSR" 200317641S
    - Form n°5 (Soutien du projet) : you have access to mesocenter `Meso@LR`
- Then you need to an account to the supercomputer:
  - "Demande de compte sur les calculateurs" -> "Demande de rattachement à un dossier de demande d'heures" and mention the project for ressources you just created.
  - "Demande de compte sur les calculateurs" -> "Demande d'ouverture de compte". At some point you will need to provide an IP address of your computer: put `194.199.235.81`, and your "FQDN (Fully Qualified Domain Name) obligatoire associé à l'adresse IP" : put `cluster-out.cirad.fr`.
- Finally, you... wait ! You will receive an email when your account is created and you project validated.

## How to use HPC ?
To efficiently use HPC, you'll need to follow a few steps.
- connect to the cluster and transfer your files
- create your computation environment in the cluster
- learn how to submit jobs

> warning
> The following steps are general and should apply to any HPC ressources. We provide below examples for `Meso@LR` cluster. However, before any action, you ___MUST___ read the documentation of the cluster you are using:
> - [Meso@LR](https://hpc-lr.umontpellier.fr/documentation-utilisateurs/)
> - [Adastra at CINES](https://dci.dci-gitlab.cines.fr/webextranet/)
> - [Jean-Zay at IDRIS](http://www.idris.fr/jean-zay/)

### Connect and transfer files to the cluster
#### Connection
##### Using a terminal
The most obvious and common way to connect to a remote machine (here your HPC cluster) is to use the [`ssh` protocol](https://www.techtarget.com/searchsecurity/definition/Secure-Shell). For exemple, for `Meso@LR`cluster, in a terminal application, you can type:

```bash
ssh -X my_login@muse-login.meso.umontpellier.fr
```

To avoid typing your password each time you connect, you can use a [ssh key](https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server).

> note
> Although considered as secure, ssh keys are sometimes not accepted on some HPC clusters. For example `adastra` does not accept ssh key.

You can [use basic linux commands](https://www.hostinger.com/tutorials/linux-commands) to navigate through the cluster.

You are now connected to the `frontal` or `login` node of the cluster. This is the node where you can submit jobs, transfer files, and prepare your computation environment. This is ___NOT___ the node where your jobs will be executed and where you can do your computations.

##### Using an IDE
You can also use an IDE (Integrated Development Environment - code editor that embeds multiple tools for developers) to connect to the cluster. For example `VSCode` provides a powerfull plugin to [develop on remote machines](https://code.visualstudio.com/docs/remote/ssh). This allows you to navigate, edit and run your code on the cluster directly from your IDE.

`PyCharm` also provides a plugin to [develop on remote machines](https://www.jetbrains.com/help/pycharm/remote-development-starting-page.html).

However, to our knowledge, no such solution exists for `RStudio`.

#### Transfer files
To run your code on the cluster, you need first to transfer your files. You can use multiple tools to transfer files:

##### `scp` & `rsync`
If you feel at ease with a terminal, you can use `scp` or `rsync` commands to transfer files. For example, to transfer a file from your computer to the cluster, you can type:

```bash
# single file
scp /where/my_file/is/my_file.txt my_login@muse-login.meso.umontpellier.fr:/where/to/put/it/
# directory
scp -r /where/my_directory/is/my_directory my_login@muse-login.meso.umontpellier.fr:/where/to/put/it/
```

This means that you have to know beforehand the path to the directory on the cluster that you want to put your files in.

[`rsync`](https://hpc-lr.umontpellier.fr/wp-content/uploads/2017/05/rsync.txt) is a more powerfull tool that allows you to synchronize directories between your computer and the cluster. It has a lot of options that you can use to customize the synchronization, and allows you to transfer only the files that have been modified since the last synchronization (very useful when you have a lot of files to transfer or when you have a slow and instable connection).

##### Filezilla
[`Filezilla`](https://filezilla-project.org/) is a client for transfering files among machines. It is very easy to use and provides a graphical interface.

You can find the "Quickconnect" bar at the top of the main window, and you need to provide the address of the "Host" : `muse-login.meso.umontpellier.fr`, your "Username" and "Password". You can also save your connection for later use : "File" -> "Copy current connection to Site Manager…".

##### Where to put my files ?
HPC clusters usually have multiple places where to put your files. Here are the ones for `Meso@LR`:
- `/home/my_login` : this is your home directory. You can put your scripts and small files here. This directory is backed up.
- `/lustre/scratch` : this is a temporary directory with very fast access where you can put your large files and data. This directory is not backed up and can be erased after 2 months. This space is recommended for running your computations that can require large amount of data.
- `/nfs/work` : this is a directory where you can put your input / output data or executable files. This directory is backed up and can be used to store your data for a long time.

### Create your computation environment
- The modules system
- Conda environments

### Submit jobs
#### The SLURM scheduler
#### How to write a SLURM script
#### How to submit and monitor a job
#### Different partitions available
- meso@lr
- adastra

# Concrete examples

## Running a Python script using GPU on meso@lr
## Install libraries and running a R script


